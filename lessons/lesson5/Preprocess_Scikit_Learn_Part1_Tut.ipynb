{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing  & Scikit Learn - Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikit learn\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets\n",
    "# --------------------------------------\n",
    "from sklearn import preprocessing\n",
    "# --------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "# --------------------------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# --------------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "# --------------------------------------\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# --------------------------------------\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# --------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# --------------------------------------\n",
    "# show several outputs in one cell. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# --------------------------------------\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bicycle rental dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description:\n",
    "* **outstanding_branch** - outstanding branch name, with highest number of rentals  \n",
    "* **popular_color** - the most popular  color of rented bicycle for the day\n",
    "* **month_number** - number of month (between 1 to 12)\n",
    "* **day_of_week** - days of week ('Sun', 'Mon', ...)\n",
    "* **working_day** - is this a working day or not\n",
    "* **temperature** - normalized temperature\n",
    "* **humidity** - normalized humidity\n",
    "* **wind_speed** - normalized wind speed\n",
    "* **number_registered** - number of registered bicycles\n",
    "* **count_rentals** - number of rented bicycles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset from csv\n",
    "Let's read the dataset and display some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bicycle_rental.csv', header=0, sep=',') \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the **'popular_color' column** functions as the target value.<br/>\n",
    "Is it a classification problem or a regression problem?<br/>\n",
    "\n",
    "We need to split the data to feature vectors and a target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'popular_color'\n",
    "y = df[target_column]\n",
    "X = df.drop([target_column],axis=1)\n",
    "X.head(3)\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Cleansing \n",
    "#### Remove rows or columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to use _boolean indexing_, and remove the data passing the condition.<br/>\n",
    "In this case we want to find all the non-working days and filter them<br/>\n",
    "\n",
    "Note that non working days includes rows which the value of \"working_day\" is 'NO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_work_days=X[(X[\"working_day\"]=='NO')]\n",
    "'rows passing condition:'\n",
    "non_work_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `drop` method** removes rows or columns (by default it removes rows).<br/>\n",
    "\n",
    "For more information goto the [pandas dataframe drop documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_work_days=X.drop(non_work_days.index)\n",
    "y_work_days=y.drop(non_work_days.index)\n",
    "'original #rows  : %d' %X.shape[0]\n",
    "'after-drop #rows: %d' %X_work_days.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how many values do the 'string' columns contain.<br/>\n",
    "\n",
    "First We find the string columns.\n",
    "Here we'll use the select_dtypes method (check the [select_dtypes documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_work_days_obj_clmns = X_work_days.select_dtypes(include='object')\n",
    "X_work_days_obj_clmns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets validate the number of values after filtering and display the values of the 'working_day' column.\n",
    "(check documentation for: [nunique](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html) and [unique](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_work_days_obj_clmns.nunique()\n",
    "X_work_days_obj_clmns['working_day'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the values (of non-working days) were removed successfully.\n",
    "\n",
    "It seems that the 'working_day' column doesn't provide (it contains only one value, let's remove it).<br/>\n",
    "Note that setting the `axis` parameter to 1 (`axis=1`), will indicate that we are dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'original #columns  : %d' %X.shape[1]\n",
    "X_work_days=X_work_days.drop('working_day',axis=1)\n",
    "'after-drop #columns: %d' %X_work_days.shape[1]\n",
    "X_work_days.head(3)\n",
    "y_work_days.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the dataset is for a learning problem and it is not splitted to train and test.<br/>\n",
    "We need to split the data to train and test and we could use the `sklearn.model_selection` module called `train_test_split` (for more information read the \n",
    "[train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))<br/>\n",
    "In order to set a reproducible result, we set a predifined random state (the `random_state` parameter).\n",
    "\n",
    "The following is an example of the usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_work_days, y_work_days, test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train.head(3)\n",
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform string values to numeric  - case 1\n",
    "Case 1 - values have an intuitive value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the original dataset and display the values of our two 'string' features: \n",
    "* 'day_of_week'\n",
    "* 'outstanding_branch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we are loading the train set:\n",
    "trainset = pd.read_csv('./data/bicycle_rental.csv', header=0, sep=',') \n",
    "\n",
    "trainset['day_of_week'].unique()\n",
    "trainset['outstanding_branch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **'day_of_week' column** could be transfered to an <u>ordinal order</u> which make sense.<br/>\n",
    "The **'outstanding_branch' column** doesn't have such an order.<br/>\n",
    "\n",
    "We could transfer 'day_of_week' values to there ordinal order in the week.<br/>\n",
    "(For more information visit the [pandas dataframe replace documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html))<br/>\n",
    "\n",
    "**'is_work_day'** - We will use the Israeli week order as a new feature and remove the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['il_day_indx'] = trainset['day_of_week'].replace({'Sun':1, 'Mon':2, 'Tue':3, 'Wed':4, 'Thu':5, 'Fri':6, 'Sat':7})\n",
    "# remove the original columns day_of_week':\n",
    "trainset = trainset.drop(['day_of_week'],axis=1)\n",
    "trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, but what about 'outstanding_branch'?<br/>\n",
    "We will need to find another solution for 'outstanding_branch' ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform string values to numeric  - case 2\n",
    "Case 2 - values don't have an intuitive value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the frequency of the **'outstanding_branch' column** using the pandas series `value_counts` function.<br/>\n",
    "(For more information check the [pandas series value_counts documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html))\n",
    "\n",
    "We call this a **histogram**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['outstanding_branch'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to visualize histograms.<br/>\n",
    "For instance, we could also visualize this using a `matplotlib` `bar` method, a series `hist` method from or an `sns` `countof` method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['outstanding_branch'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we don't have an order in a categorical (string here) value, we could check for existence of the value.<br/>\n",
    "The **pandas `get_dummies` method** does just that, by transferring each possible string value to a binary feature.<br/>\n",
    "(For more information check the [pandas get_dummies documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html))\n",
    "\n",
    "Let's apply this for the 'outstanding_branch' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outstanding_branchs = pd.get_dummies(trainset['outstanding_branch'],prefix='branch_',drop_first=False)\n",
    "df_binary = pd.concat([trainset,outstanding_branchs],axis=1).drop('outstanding_branch',axis=1)\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer continues numeric values to discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options to turn continues values to discrete ones.<br/>\n",
    "Actually the feature values don't have to be continues.<br/>\n",
    "\n",
    "We might want to divide int values to groups.<br/>\n",
    "The **pandas `qcut` method**, does this by dividing the values into quantiles (For more information check the [pandas qcut documentation](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html)).\n",
    "\n",
    "Let's do this for the 'count_rentals' feature (number of rented bicycles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary['count_rentals'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary['count_rentals'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary['count_rentals'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary['count_rentals'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_descrete=pd.qcut(df_binary['count_rentals'], q=4,labels=[0,1,2,3])\n",
    "cnt_descrete.name='cnt_descrete'\n",
    "df_descrete = pd.concat([df_binary,cnt_descrete],axis=1).drop('count_rentals',axis=1)\n",
    "df_descrete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise start with the original dataframe df.<br/>\n",
    "1. Create a new feature (column) called 'is_work_time', which could contain 2 values:\n",
    "  * 0 (for non-working time)\n",
    "  * 1: for working time \n",
    "  * We consider Non working time, as one of the following (or more):  \n",
    "    * a value of 'NO' for the 'working_day' column\n",
    "    * Month number 7 or 8 (Summer month - July or August) for the 'month_number' column \n",
    "  * After creating this new column, remove the 'working_day' and the 'month_number' columns.\n",
    "2. Create a 'work_day_order' column, containing the work day order as following:\n",
    "  * 1 - beginning of week, including 'Sun' and 'Mon'\n",
    "  * 2 - middle of the week, including 'Tue', 'Wed', 'Thu'\n",
    "  * 3 - End of week, including 'Fri' and 'Sat'\n",
    "  * Remove the original 'day_of_week' column.\n",
    "3. Discretize 'popular_color' on existence bases of the colors (exists = 1, doesn't exist = 0).\n",
    "   * Use 'is_color' as a prefix\n",
    "4. Discretize 'wind_speed' into 5 parts, using the pandas qcut command \n",
    "   * The new column should be called 'wind_discrete'\n",
    "   * remove the 'wind_speed' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we are loading the train set:\n",
    "trainset = pd.read_csv('./data/bicycle_rental.csv', header=0, sep=',') \n",
    "trainset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - part 1 your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - part 2 your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - part 3 your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - part 4 your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform standardization with the `sklearn.preprocessing` module's `StandardScaler` (For more information visit:  [StandardScaler documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n",
    "\n",
    "The following cells demonstreate a usage example for `StandardScaler`, on our dataset.\n",
    "\n",
    "Let's first reload the information and select some numeric columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bicycle_rental.csv') \n",
    "target_column = 'popular_color'\n",
    "y = df[target_column]\n",
    "X = df[['temperature', 'humidity', 'wind_speed']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "X_train_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could visualize the pdf (probability density function change) with `sns.kdeplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1.set_title('Before Scaling')\n",
    "ax1.set_xlabel('features')\n",
    "sns.kdeplot(X_train.iloc[:,0], ax=ax1)\n",
    "sns.kdeplot(X_train.iloc[:,1], ax=ax1)\n",
    "sns.kdeplot(X_train.iloc[:,2], ax=ax1)\n",
    "\n",
    "ax2.set_title('After Standard Scaler')\n",
    "ax2.set_xlabel('features')\n",
    "sns.kdeplot(X_train_scaled.iloc[:,0], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled.iloc[:,1], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled.iloc[:,2], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For minmax normalization we could use the `sklearn.preprocessing` `MinMaxScaler` (For more information visit the [MinMaxScaler documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "The following is a usage example for `MinMaxScaler`, on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_minmax = pd.DataFrame(minmax_scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_minmax = pd.DataFrame(minmax_scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "X_train_minmax.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "ax1.set_title('Before Scaling')\n",
    "ax1.set_xlabel('features')\n",
    "sns.kdeplot(X_train.iloc[:,0], ax=ax1)\n",
    "sns.kdeplot(X_train.iloc[:,1], ax=ax1)\n",
    "sns.kdeplot(X_train.iloc[:,2], ax=ax1)\n",
    "\n",
    "ax2.set_title('After Min-Max[-1,1] Scaler')\n",
    "ax2.set_xlabel('features')\n",
    "sns.kdeplot(X_train_minmax.iloc[:,0], ax=ax2)\n",
    "sns.kdeplot(X_train_minmax.iloc[:,1], ax=ax2)\n",
    "sns.kdeplot(X_train_minmax.iloc[:,2], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient is given as a build in 'corr' dataframe (for more information visit the [pandas dataframe corr documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)).<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv('./data/bicycle_rental.csv') \n",
    "df_numeric = trainset.select_dtypes(include=np.number)\n",
    "df_numeric.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a visualization, with **the `sns` `heatmap` function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = trainset._get_numeric_data()\n",
    "\n",
    "mask = np.zeros_like(df_numeric.corr(),dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(10,150,as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(df_numeric.corr(), cmap=cmap, annot=True, mask=mask, square=True, center=0)\n",
    "plt.title('Pearson Correlation',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide to remove features with **correlation > 0.8**, we need to **remove either 'number_registered' or 'count_rentals'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could perform dimensionality reduction, instead or in addition to feature selection.<br/>\n",
    "\n",
    "We will do this using the `sklearn.decomposition`'s `PCA` method (for more information visit the [sickit learn's PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)).<br/>\n",
    "\n",
    "We could decide about the number of principal components, using the `n_components` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df_numeric)\n",
    "\n",
    "pca.explained_variance_\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the parameter of the number of components to be less than one, set's the minimum number of components for a >= information of the given value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9)\n",
    "reduced_data = pd.DataFrame(pca.fit_transform(df_numeric),columns=['x1'],index=df_numeric.index)\n",
    "reduced_data.shape[1]\n",
    "reduced_data.head(3)\n",
    "pca.explained_variance_\n",
    "pca.explained_variance_ratio_\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Feature Engineering (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will work on the iris dataset.\n",
    "1. fit minmax scaler, between [0,1]  \n",
    "  * fit and apply on train set\n",
    "  * apply on test set\n",
    "2. Detect correlated features on the scaled train set.\n",
    "   * Display the correlation\n",
    "   * assuming correlated values have a correlation of > 0.9, are there any correlations, neede to be filtered?\n",
    "3. Perform dimensionality reduction, using PCA\n",
    "   * Use components covering at least 0.9 of the information\n",
    "   * how many components were used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the iris dataset\n",
    "iris_dataset =datasets.load_iris()\n",
    "X = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "y = pd.Series(iris_dataset.target)\n",
    "y.name = 'target'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "X_train.head(3)\n",
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2, part 1, your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2, part 2, your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2, part 3, your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
