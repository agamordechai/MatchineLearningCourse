{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: \n",
    "Gradient Boosting is an ensemble learning technique that builds a strong predictive model by combining the predictions of multiple weaker models, typically decision trees. Unlike Random Forest, which builds trees independently, Gradient Boosting builds trees sequentially, each one correcting the errors of its predecessor.\n",
    "\n",
    "The key idea is to optimize a loss function over the iterations:\n",
    "1. Initialize the model with a constant value.\n",
    "2. Fit a weak learner (e.g., a decision tree) to the residuals (errors) of the previous model.\n",
    "3. Update the model by adding the fitted weak learner to minimize the loss.\n",
    "4. Repeat the process for a specified number of iterations or until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Age': [25, 45, 35, 50, 23, 37, 32, 28, 40, 27],\n",
    "    'Income': [50000, 60000, 70000, 80000, 20000, 30000, 40000, 55000, 65000, 75000],\n",
    "    'Years_Experience': [1, 20, 10, 25, 2, 5, 7, 3, 15, 12],\n",
    "    'Loan_Approved': [0, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Age', 'Income', 'Years_Experience']]\n",
    "y = df['Loan_Approved']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the gradient boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "print(f\"Feature Importances:\\n{feature_importances}\")\n",
    "\n",
    "# Plotting the feature importances\n",
    "sns.barplot(x=feature_importances.index, y=feature_importances['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We create a DataFrame containing features (Age, Income, Years_Experience) and the target variable (Loan_Approved).\n",
    "3. Feature and Target: We separate the features and the target variable.\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a GradientBoostingClassifier model with 100 estimators (n_estimators=100), a learning rate of 0.1, and a maximum depth of 3, and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict loan approval for the test set.\n",
    "7. Evaluation: We evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "8. Feature Importance: We compute and display the importance of each feature.\n",
    "9. Visualization: We plot the feature importances to visualize which features contribute most to the model's predictions.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- Accuracy: The proportion of correctly classified instances among the total instances.\n",
    "- Confusion Matrix: Counts of TP, TN, FP, and FN.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type: Gradient Boosting builds Decision Trees. \n",
    "\n",
    "#### Gradient Boosting, XGBoost, LightGBM, Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
