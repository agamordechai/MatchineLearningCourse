{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Concept: \n",
    "XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for speed and performance. It builds an ensemble of decision trees sequentially, where each tree corrects the errors of its predecessor. XGBoost is known for its scalability, efficiency, and flexibility, and is widely used in machine learning competitions and real-world applications.\n",
    "\n",
    "#### Key Features of XGBoost\n",
    "1. Regularization: Helps prevent overfitting by penalizing complex models.\n",
    "2. Parallel Processing: Speeds up training by utilizing multiple cores of a CPU.\n",
    "3. Handling Missing Values: Automatically handles missing data by learning which path to take in a tree.\n",
    "4. Tree Pruning: Uses a depth-first approach to prune trees more effectively.\n",
    "5. Built-in Cross-Validation: Integrates cross-validation to optimize the number of boosting rounds.\n",
    "\n",
    "#### Key Steps\n",
    "1. Define the Objective Function: This is the loss function to be minimized.\n",
    "2. Compute Gradients: Calculate the gradients of the loss function.\n",
    "3. Fit the Trees: Train decision trees to predict the gradients.\n",
    "4. Update the Model: Combine the predictions of all trees to make the final prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and xgboost.\n",
    "2. Data Preparation: We load the Breast Cancer dataset with features and the target variable (malignant or benign).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Model Training: We create an XGBClassifier model and train it using the training data.\n",
    "5. Predictions: We use the trained XGBoost model to predict the labels for the test set.\n",
    "6. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "\n",
    "XGBoost is widely used in various fields such as:\n",
    "- Finance: Fraud detection, credit scoring.\n",
    "- Healthcare: Disease prediction, patient risk stratification.\n",
    "- Marketing: Customer segmentation, churn prediction.\n",
    "- Sports: Player performance prediction, match outcome prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### type: XGBoost (Extreme Gradient Boosting) builds Decision Trees. \n",
    "\n",
    "#### Gradient Boosting, XGBoost, LightGBM, Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
