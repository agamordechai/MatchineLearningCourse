{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to recognize patterns in sequences of data such as time series, natural language, or video frames. Unlike traditional neural networks, RNNs have connections that form directed cycles, allowing them to maintain a hidden state that can capture information about previous inputs.\n",
    "\n",
    "#### Key Features of RNNs\n",
    "1. Sequential Data Processing: Designed to handle sequences of varying lengths.\n",
    "2. Hidden State: Maintains information about previous elements in the sequence.\n",
    "3. Shared Weights: Uses the same weights across all time steps, reducing the number of parameters.\n",
    "4. Vanishing/Exploding Gradient Problem: Can struggle with long-term dependencies due to these issues.\n",
    "\n",
    "#### Key Steps\n",
    "1. Input and Hidden States: Each input element is processed along with the hidden state from the previous time step.\n",
    "2. Recurrent Connections: The hidden state is updated recursively.\n",
    "3. Output Layer: Produces predictions based on the hidden state at each time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generate synthetic sequential data\n",
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "\n",
    "# Prepare the dataset\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step)]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Create the dataset with time steps\n",
    "time_step = 10\n",
    "X, y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(time_step, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Predict the next value in the sequence\n",
    "last_sequence = X_test[-1].reshape(1, time_step, 1)\n",
    "predicted_value = model.predict(last_sequence)\n",
    "predicted_value = scaler.inverse_transform(predicted_value)\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code\n",
    "\n",
    "1. Data Generation: We generate synthetic sequential data using a sine function.\n",
    "2. Dataset Preparation: We create sequences of 10 time steps to predict the next value.\n",
    "3. Data Scaling: Normalize the data to the range [0, 1] using MinMaxScaler.\n",
    "4. Dataset Creation: Create the dataset with input sequences and corresponding labels.\n",
    "5. Train-Test Split: Split the data into training and test sets.\n",
    "6. Model Creation:\n",
    "   - SimpleRNN Layer: A recurrent layer with 50 units.\n",
    "   - Dense Layer: A fully connected layer with a single output neuron for regression.\n",
    "7. Model Compilation: We compile the model with the Adam optimizer and mean squared error loss function.\n",
    "8. Model Training: Train the model for 50 epochs with a batch size of 1.\n",
    "9. Model Evaluation: Evaluate the model on the test set and print the loss.\n",
    "10. Prediction: Predict the next value in the sequence using the last sequence from the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Features of RNNs\n",
    "\n",
    "1. LSTM (Long Short-Term Memory): Designed to handle long-term dependencies better than vanilla RNNs.\n",
    "2. GRU (Gated Recurrent Unit): A simplified version of LSTM with similar performance.\n",
    "3. Bidirectional RNNs: Process the sequence in both forward and backward directions.\n",
    "4. Stacked RNNs: Use multiple layers of RNNs for better feature extraction.\n",
    "5. Attention Mechanisms: Improve the model's ability to focus on important parts of the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with LSTM\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(time_step, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate the model (same as before)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "\n",
    "RNNs are widely used in various fields such as:\n",
    "- Natural Language Processing (NLP): Language modeling, machine translation, text generation.\n",
    "- Time Series Analysis: Stock price prediction, weather forecasting, anomaly detection.\n",
    "- Speech Recognition: Transcribing spoken language into text.\n",
    "- Video Analysis: Activity recognition, video captioning.\n",
    "- Music Generation: Composing music by predicting sequences of notes.\n",
    "\n",
    "RNNs' ability to capture temporal dependencies makes them highly effective for sequential data tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
